---
title: Accelerating MinuerU Multimodal Inference with LightLLM
tags:
- MTC Team
- New Feature
excerpt: |
  LightLLM now provides optimized support for the MinuerU multimodal model: we reduce RPyC communication overhead, speed up image preprocessing, and refine ViT batching and downstream dispatch to significantly improve end-to-end performance.
---

In LightLLM, multimodal inference consists of two stages: first, input images are preprocessed and fed into the vision encoder to obtain image embeddings; then these embeddings are concatenated with text embeddings and passed into the LLM for generation.

While integrating MinuerU, we optimized the communication layer, refactored ViT batching and dispatch logic, and streamlined the image preprocessing pipeline. These changes yield clear performance gains across different resolutions and hardware setups.

## MinuerU Multimodal Inference Flow in LightLLM

1. **Image preprocessing** (resize, normalization, and other operations based on the visual spec).
2. Use **RPyC to call the remote ViT** and generate image embeddings.
3. **Embedding fusion**: concatenate image embeddings with text embeddings.
4. **LLM decoding**: feed the combined sequence into the LLM for generation.

During integration, we noticed that the TCP behavior of RPyC, along with the tight coupling between ViT batch size and downstream dispatch, was a major source of latency—especially for small images and high-concurrency workloads.

## Reducing RPyC Overhead with `TCP_NODELAY`

Because TCP enables Nagle’s algorithm by default, small packets can be coalesced, introducing extra wait time for certain RPyC calls. We explicitly enable **`TCP_NODELAY`** on the RPyC connection to avoid this.

On an H200, using **448×448** low-resolution images in batch inference, we observed that enabling `TCP_NODELAY` **cut latency roughly in half**.

Performance comparison:

| Metric | LightLLM-before | LightLLM-opti |
|:--|:--:|:--:|
| QPS (req/s) | 30.20 | 63.03 |
| Prefill P50 (s) | 3.70 | 2.80 |
| Decode P50 (ms) | 30.0 | 28.0 |

This change is especially beneficial in scenarios with many small requests or strict latency SLAs.

## Optimizing ViT Batching and Dispatch Behavior

Previously, the ViT batch size was fully determined by `infer_batch_size`: the pipeline accumulated images in `images_need_infer`, triggered `infer_imgs` once the threshold was reached, and **immediately** dispatched the associated requests downstream. This led to:

- **Strong coupling** between **ViT batch size** and **downstream dispatch pattern**.
- A burst of small `send_pyobj` calls after each ViT inference, amplifying RPyC overhead.

We refactored the main loop logic to **decouple ViT batching from request dispatch**. This allows us to:

- **Reduce the number of small-granularity RPyC messages**, amortizing `send_pyobj` overhead.
- Use `infer_batch_size` to keep ViT highly utilized, while `send_batch_size` makes it easier for downstream LLM prefill to form efficient batches.
- Lower **end-to-end latency jitter** under high concurrency.

## Speeding Up Image Preprocessing

We also streamlined several steps in **image preprocessing**, reducing CPU overhead before the vision encoder. This further improves end-to-end latency, particularly in high-concurrency benchmarks where preprocessing becomes a bottleneck.

## Performance Evaluation

We benchmarked LightLLM (with MinuerU integrated) against vLLM on different hardware, using the same model and comparable configurations.

### H200, 1080×1080 Resolution, 10 Concurrency

| Metric | vLLM | LightLLM |
|:--|:--:|:--:|
| QPS (req/s) | 7.29 | 8.38 |
| Prefill P50 (s) | 1.11 | 1.06 |
| Decode P50 (ms) | 5.4 | 6.4 |

### RTX 4090D, 1080×1080 Resolution, 10 Concurrency

| Metric | vLLM | LightLLM |
|:--|:--:|:--:|
| QPS (req/s) | 1.40 | 1.61 |
| Prefill P50 (s) | 1.14 | 2.26 |
| Prefill P100 (s) | 5.65 | 3.23 |
| Decode P50 (ms) | 5.88 | 3.79 |

Overall, the results show that **MinuerU on LightLLM achieves slightly higher QPS than on vLLM** under comparable settings, while the optimized communication, batching, and preprocessing strategies help stabilize and improve end-to-end performance.
